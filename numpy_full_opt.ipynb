{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc3de08-88be-469e-b666-9f43457246c4",
   "metadata": {},
   "source": [
    "# Brute Force with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9663d273-9e91-4410-a674-7996d3f970a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082a9b42-87a4-4875-b94d-ae25c967c174",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3190d2c3-b54e-4c03-b528-0dc85a1f0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wordle-candidates.json', 'r') as file:\n",
    "    wordle_candidates = json.load(file)\n",
    "    \n",
    "with open('data/wordle-answers.json', 'r') as file:\n",
    "    wordle_answers = json.load(file)\n",
    "\n",
    "wordle_candidates = pd.DataFrame(wordle_candidates['words'], columns=['word'])\n",
    "wordle_answers = pd.DataFrame(wordle_answers['words'], columns=['word'])\n",
    "wordle_candidates['is_answer'] = 0\n",
    "wordle_answers['is_answer'] = 1\n",
    "wordle = wordle_candidates.append(wordle_answers).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4273010f-6e69-4305-9c65-35216873bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_all = pd.read_table('data/archive/en_words_1_5-5.txt', delimiter=' ', header=None, index_col=None,\n",
    "                         names=['word_len', 'word_freq', 'n_articles']).reset_index()\n",
    "words_all = words_all.rename(columns={'index': 'word'})\n",
    "\n",
    "# Filter by english\n",
    "alphabet = list('abcdefghijklmnopqrstuvwxyz')\n",
    "words_all = words_all.loc[words_all.word.apply(lambda x: all([l in alphabet for l in x]))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5459e01-8713-4ac4-a1e6-b43a13f74a83",
   "metadata": {},
   "source": [
    "## Prepare Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b4cf8e6-7c7d-44d7-9a8b-d1848af67039",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_dict = {l: i for i, l in enumerate(list('abcdefghijklmnopqrstuvwxyz'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c4520c3-aae9-4e40-9594-0dc58547ef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise solutions vector\n",
    "solutions = np.zeros((wordle_answers.shape[0], 26, 5), dtype='int8')\n",
    "for i, word in enumerate(wordle_answers.word):\n",
    "    for j, l in enumerate(word):\n",
    "        solutions[i, alpha_dict[l], j] = 1\n",
    "        \n",
    "        \n",
    "# Initialise candidates vector\n",
    "candidates = np.zeros((wordle.shape[0], 26, 5), dtype='int8')\n",
    "for i, word in enumerate(wordle.word):\n",
    "    for j, l in enumerate(word):\n",
    "        candidates[i, alpha_dict[l], j] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c586369-af5f-4976-aac0-69e826034077",
   "metadata": {},
   "source": [
    "## Game Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab7770d4-9ac9-4b66-836d-1210f6f93bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feedback(input_word, solution):\n",
    "    output = ''\n",
    "    for i in range(5):\n",
    "        if input_word[i] == solution[i]:\n",
    "            output += 'G'\n",
    "        elif input_word[i] in solution:\n",
    "            output += 'Y'\n",
    "        else:\n",
    "            output += 'X'\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fda5f160-5378-4e6e-b566-229400f39fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_wordset(input_word, feedback, wordset):\n",
    "    newset = wordset.copy()\n",
    "    for i in range(5):\n",
    "        if feedback[i] == 'G':\n",
    "            newset = newset.loc[newset.word.str[i] == input_word[i]]\n",
    "        elif feedback[i] == 'Y':\n",
    "            # newset = newset.loc[newset.word.str.contains(input_word[i])]\n",
    "            newset = newset.loc[newset.word.str.contains(input_word[i]) & newset.word.apply(lambda x: x[i] != input_word[i])]\n",
    "        else:\n",
    "            newset = newset.loc[~newset.word.str.contains(input_word[i])]\n",
    "    return newset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779dfd9c-9d44-4f00-8831-e0f78388f1b2",
   "metadata": {},
   "source": [
    "## Vector Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5a99dbf-e831-4453-9b17-5a547702d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_vec(word):\n",
    "    mat = np.zeros((26, 5), dtype='int8')\n",
    "    for i, l in enumerate(word):\n",
    "        mat[alpha_dict[l], i] = 1\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03a150ed-a3e3-46ec-972d-309de010dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(word, mask):\n",
    "    word_vec = init_vec(word)\n",
    "    solutions_masked = solutions[mask]\n",
    "    greens = solutions_masked * word_vec\n",
    "    yellows = word_vec * (\n",
    "        (solutions_masked.sum(axis=2) >= word_vec.sum(axis=1)) & \n",
    "        (word_vec.sum(axis=1) > 0)) \\\n",
    "        .reshape(np.sum(mask), 26, 1) - greens\n",
    "    greys = word_vec - greens - yellows\n",
    "    scores = np.array([np.sum(greens, axis=(1,2)), np.sum(yellows, axis=(1,2)), np.sum(greys, axis=(1,2))]).T\n",
    "    # scores = []\n",
    "    # for i in np.array(range(solutions.shape[0]))[mask]:\n",
    "    #     solution = solutions[i]\n",
    "    #     greens = solution * word_vec\n",
    "    #     yellows = word_vec * ((solution.sum(axis=1) >= word_vec.sum(axis=1)) & (word_vec.sum(axis=1) > 0)).reshape(26, 1) - greens\n",
    "    #     greys = word_vec - greens - yellows\n",
    "    #     scores.append((np.sum(greens), np.sum(yellows), np.sum(greys)))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e4c1447-69d4-46f5-966b-52766537e19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_scores(word, mask):\n",
    "    scores = get_scores(word, mask)\n",
    "    df_scores = pd.DataFrame(scores, columns=['g', 'y', 'x'])\n",
    "    df_scores['score'] = df_scores.g * 2 + df_scores.y\n",
    "    \n",
    "    return df_scores.score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdbb6368-76e9-4e68-8caa-154381515466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_cands(word, filter_mask, candidate_mask):\n",
    "    return np.sum((np.sum((filter_mask * solutions[candidate_mask]) == solutions[candidate_mask], axis=(-2,-1)) == 130))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5e27a25-bd47-4b12-a0c5-a9dcb52153d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 4],\n",
       "       [0, 0, 5],\n",
       "       [1, 0, 4],\n",
       "       ...,\n",
       "       [1, 1, 3],\n",
       "       [0, 1, 4],\n",
       "       [0, 2, 3]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scores('happy', [True] * 2315)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8883e64-f7e7-48d5-bf9c-870efc89777a",
   "metadata": {},
   "source": [
    "### Filter Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51b37f1e-bb94-4774-8eb4-1a453aedde3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_candidate_mask():\n",
    "    return np.array([True] * wordle_answers.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4fc40c1-16e2-4f5e-9366-4bb308fe412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_filter_mask():\n",
    "    filter_mask = np.ones((26,5))\n",
    "    return filter_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7da9dfb-cd96-439b-884e-4be7edc4482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_filter_mask(input_word, feedback, mask):\n",
    "    wv = init_vec(input_word)\n",
    "    row_idx = [alpha_dict[l] for l in input_word]\n",
    "    output = mask.copy()\n",
    "    for i, (fb, r) in enumerate(zip(feedback, row_idx)):\n",
    "        # Green\n",
    "        if fb == 'G':\n",
    "            output[:, i] = 0\n",
    "            output[r, i] = 1\n",
    "        # Yellow\n",
    "        elif fb == 'Y':\n",
    "            output[r, i] = 0\n",
    "        # Grey\n",
    "        elif fb == 'X':\n",
    "            output[r, :] = 0\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e9f6e76-b66d-404e-858f-33ce3aa0b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_candidate_mask(input_word, feedback, wordset, mask):\n",
    "    newmask = mask.copy()\n",
    "    for i in range(5):\n",
    "        if feedback[i] == 'G':\n",
    "            newmask[~wordset.word.str[i].eq(input_word[i])] = False\n",
    "        elif feedback[i] == 'Y':\n",
    "            newmask[~(wordset.word.str.contains(input_word[i]) & wordset.word.apply(lambda x: x[i] != input_word[i]))] = False\n",
    "        elif feedback[i] == 'X':\n",
    "            newmask[wordset.word.str.contains(input_word[i])] = False\n",
    "            \n",
    "    return newmask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafbdfee-473f-4b6b-b9b3-606cc01dca1c",
   "metadata": {},
   "source": [
    "## Global Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a8c4ddd-0ff0-4313-8a35-90eb9eb87fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_letter_frequencies(wordset):\n",
    "    w = wordset.copy()\n",
    "    for letter in list('abcdefghijklmnopqrstuvwxyz'):\n",
    "        w[letter] = w.word.str.contains(letter).astype(int)\n",
    "    return w.iloc[:, 1:]\n",
    "\n",
    "def compute_score(x, freqs):\n",
    "    letters = set(x)\n",
    "    output = 0\n",
    "    for letter in letters:\n",
    "        output += freqs[letter]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "390fcb23-57d8-4953-a166-4ce9f5d1ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_freqs = compute_letter_frequencies(wordle).sum().to_dict()\n",
    "global_scores = wordle.word.apply(compute_score, freqs=global_freqs)\n",
    "global_scores = pd.DataFrame({'word': wordle.word, 'score': global_scores})\n",
    "global_scores = global_scores.merge(words_all[['word', 'word_freq', 'n_articles']], how='left', left_on='word', right_on='word')\n",
    "global_scores = global_scores.fillna(0).sort_values(['score', 'word_freq'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a4090e-de52-4843-9b65-87cf47be6be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "df3d045a-f3b1-4d1f-b7a1-0b7bd2374be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute number of candidates based on GYX\n",
    "def compute_cands(gyx_triplet):\n",
    "    \n",
    "    gyx_triplet = gyx_triplet.reshape(3,26,5)\n",
    "    \n",
    "    # Green checks\n",
    "    if np.sum(gyx_triplet[0]) > 0:\n",
    "        green_boolean = np.sum(gyx_triplet[0] * solutions == gyx_triplet[0], axis=(-2,-1)) == 130\n",
    "        filtered_solutions = solutions[green_boolean]\n",
    "    else:\n",
    "        filtered_solutions = solutions.copy()\n",
    "    \n",
    "    # Yellow avoid: All yellow locations are zero\n",
    "    if np.sum(gyx_triplet[1]) > 0:\n",
    "        yellow_avoid = np.sum(gyx_triplet[1] * filtered_solutions == 0, axis=(-2,-1)) == 130\n",
    "\n",
    "        # Yellow present: \n",
    "        # 1. Compute row sums for yellow vector\n",
    "        # 2. Select rows with at least one yellow in each solution word vector\n",
    "        # 3. Compute row sums for solution vector to check there are at least one\n",
    "        # 4. Check that there are two\n",
    "        yellow_sums = np.sum(gyx_triplet[1], axis=-1)\n",
    "        yellow_present = np.sum(\n",
    "            np.sum(filtered_solutions[:, yellow_sums >= 1, :], axis=-1) >= 1,\n",
    "            axis=-1) == 2\n",
    "\n",
    "        # Combine yellow checks\n",
    "        yellow_boolean = yellow_present * yellow_avoid\n",
    "\n",
    "        # Filter based on yellows\n",
    "        filtered_solutions = filtered_solutions[yellow_boolean]\n",
    "\n",
    "    # Grey checks\n",
    "    if np.sum(gyx_triplet[2]) > 0:\n",
    "        grey_boolean = np.sum(np.sum(gyx_triplet[2], axis=-1, keepdims=True) * filtered_solutions == 0, axis=(-2,-1)) == 130\n",
    "        filtered_solutions = filtered_solutions[grey_boolean]\n",
    "\n",
    "    # Count no. of candidates\n",
    "    return filtered_solutions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "33962b52-939f-4f7e-9045-ea2c223b07b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ncands(word):\n",
    "    # Initialise candidate word vector\n",
    "    word_vec = init_vec(word)\n",
    "\n",
    "    # Compute greens, yellows, and greys\n",
    "    greens = candidates * word_vec\n",
    "    yellows = word_vec * (\n",
    "        (candidates.sum(axis=2) >= word_vec.sum(axis=1)) & \n",
    "        (word_vec.sum(axis=1) > 0)) \\\n",
    "        .reshape(12972, 26, 1) - greens\n",
    "    greys = word_vec - greens - yellows\n",
    "\n",
    "    # Set up GYX tensor\n",
    "    gyx_reshaped = np.stack([greens, yellows, greys], axis=1)\n",
    "    gyx_reshaped = gyx_reshaped.reshape(12972, 390)\n",
    "    \n",
    "    # Compute raw candidate data\n",
    "    ncands = np.apply_along_axis(compute_cands, 1, gyx_reshaped)\n",
    "    ncands_max = np.max(ncands)\n",
    "    ncands_mean = np.mean(ncands)\n",
    "    \n",
    "    return word, ncands_max, ncands_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2588e0f4-dd8f-463f-b734-a26aefaefe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4d42da1c-48a3-492a-9cfb-6706f726b148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.319398403167725\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "z1 = get_ncands('anana')\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f8198160-8189-40e7-a657-fe8c36861b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('anana', 1054, 785.949892075239)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bbb14c-0f87-4acd-b57f-eafd75f42e18",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "83eb8ec5-561c-477c-935b-7e73f3b3cc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=5)]: Done 500 out of 500 | elapsed: 16.1min finished\n"
     ]
    }
   ],
   "source": [
    "test1 = Parallel(n_jobs=5, verbose=1)(delayed(get_ncands)(word) for word in global_scores.word.iloc[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "47bc3df7-5c05-42a2-b2b0-a04125cd328b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ncands_max</th>\n",
       "      <th>ncands_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>sauce</td>\n",
       "      <td>243</td>\n",
       "      <td>40.464770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>noise</td>\n",
       "      <td>254</td>\n",
       "      <td>44.387835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>sayne</td>\n",
       "      <td>256</td>\n",
       "      <td>38.804502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>saute</td>\n",
       "      <td>276</td>\n",
       "      <td>38.646315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>saice</td>\n",
       "      <td>278</td>\n",
       "      <td>36.816220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>toise</td>\n",
       "      <td>280</td>\n",
       "      <td>43.883287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>salue</td>\n",
       "      <td>284</td>\n",
       "      <td>40.062673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>saine</td>\n",
       "      <td>288</td>\n",
       "      <td>36.460762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>cause</td>\n",
       "      <td>300</td>\n",
       "      <td>46.460299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>pause</td>\n",
       "      <td>302</td>\n",
       "      <td>50.530065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  ncands_max  ncands_mean\n",
       "461  sauce         243    40.464770\n",
       "245  noise         254    44.387835\n",
       "319  sayne         256    38.804502\n",
       "141  saute         276    38.646315\n",
       "137  saice         278    36.816220\n",
       "197  toise         280    43.883287\n",
       "129  salue         284    40.062673\n",
       "42   saine         288    36.460762\n",
       "460  cause         300    46.460299\n",
       "474  pause         302    50.530065"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(test1, columns=['word', 'ncands_max', 'ncands_mean'])\n",
    "df1.sort_values('ncands_max').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fdcf23db-3246-46d0-83f7-959c7db46098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['rank_max'] = df1.ncands_max.rank()\n",
    "df1['rank_mean'] = df1.ncands_mean.rank()\n",
    "df1['avg_rank'] = df1[['rank_max', 'rank_mean']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8bef602e-99c2-4532-be2b-08647b1e125e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ncands_max</th>\n",
       "      <th>ncands_mean</th>\n",
       "      <th>rank_max</th>\n",
       "      <th>rank_mean</th>\n",
       "      <th>avg_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>saice</td>\n",
       "      <td>278</td>\n",
       "      <td>36.816220</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>saute</td>\n",
       "      <td>276</td>\n",
       "      <td>38.646315</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>sayne</td>\n",
       "      <td>256</td>\n",
       "      <td>38.804502</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>saine</td>\n",
       "      <td>288</td>\n",
       "      <td>36.460762</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>sauce</td>\n",
       "      <td>243</td>\n",
       "      <td>40.464770</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>salue</td>\n",
       "      <td>284</td>\n",
       "      <td>40.062673</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>claes</td>\n",
       "      <td>321</td>\n",
       "      <td>40.606075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aloes</td>\n",
       "      <td>364</td>\n",
       "      <td>34.039624</td>\n",
       "      <td>36.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lares</td>\n",
       "      <td>370</td>\n",
       "      <td>40.109235</td>\n",
       "      <td>43.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>28.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>salet</td>\n",
       "      <td>353</td>\n",
       "      <td>41.461918</td>\n",
       "      <td>30.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  ncands_max  ncands_mean  rank_max  rank_mean  avg_rank\n",
       "137  saice         278    36.816220       5.0        3.0      4.00\n",
       "141  saute         276    38.646315       4.0        4.0      4.00\n",
       "319  sayne         256    38.804502       3.0        5.0      4.00\n",
       "42   saine         288    36.460762       8.0        2.0      5.00\n",
       "461  sauce         243    40.464770       1.0       15.0      8.00\n",
       "129  salue         284    40.062673       7.0       12.0      9.50\n",
       "222  claes         321    40.606075      13.0       17.0     15.00\n",
       "8    aloes         364    34.039624      36.5        1.0     18.75\n",
       "12   lares         370    40.109235      43.0       13.0     28.00\n",
       "66   salet         353    41.461918      30.0       29.0     29.50"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.sort_values('avg_rank').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c40d5838-0644-4546-8462-d2512ef920f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:  4.3min finished\n"
     ]
    }
   ],
   "source": [
    "test2 = Parallel(n_jobs=5, verbose=1)(delayed(get_ncands)(word) for word in wordle.word.iloc[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8e766da1-fba3-42b8-83f2-2a04054e2cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ncands_max</th>\n",
       "      <th>ncands_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>abune</td>\n",
       "      <td>344</td>\n",
       "      <td>66.645698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>aboil</td>\n",
       "      <td>348</td>\n",
       "      <td>63.762026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>aceta</td>\n",
       "      <td>359</td>\n",
       "      <td>114.152559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>actin</td>\n",
       "      <td>378</td>\n",
       "      <td>99.115171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>acids</td>\n",
       "      <td>383</td>\n",
       "      <td>64.878122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>addio</td>\n",
       "      <td>390</td>\n",
       "      <td>82.710762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>ached</td>\n",
       "      <td>390</td>\n",
       "      <td>91.699198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>adieu</td>\n",
       "      <td>394</td>\n",
       "      <td>48.431545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>acold</td>\n",
       "      <td>400</td>\n",
       "      <td>96.866019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>adred</td>\n",
       "      <td>409</td>\n",
       "      <td>110.192800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>abbes</td>\n",
       "      <td>423</td>\n",
       "      <td>89.563136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>acari</td>\n",
       "      <td>426</td>\n",
       "      <td>154.538545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>acyls</td>\n",
       "      <td>427</td>\n",
       "      <td>80.046099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>acnes</td>\n",
       "      <td>427</td>\n",
       "      <td>48.570459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>acted</td>\n",
       "      <td>428</td>\n",
       "      <td>70.731190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>absey</td>\n",
       "      <td>430</td>\n",
       "      <td>68.383287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>abear</td>\n",
       "      <td>431</td>\n",
       "      <td>138.623034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>addle</td>\n",
       "      <td>434</td>\n",
       "      <td>121.280296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>adbot</td>\n",
       "      <td>434</td>\n",
       "      <td>100.679078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>abrin</td>\n",
       "      <td>436</td>\n",
       "      <td>87.739053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>abore</td>\n",
       "      <td>437</td>\n",
       "      <td>63.105535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aahed</td>\n",
       "      <td>448</td>\n",
       "      <td>146.204595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>abrim</td>\n",
       "      <td>454</td>\n",
       "      <td>101.562365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>adoze</td>\n",
       "      <td>454</td>\n",
       "      <td>78.151249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>acidy</td>\n",
       "      <td>455</td>\n",
       "      <td>116.479494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>acmes</td>\n",
       "      <td>455</td>\n",
       "      <td>57.047410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>abele</td>\n",
       "      <td>457</td>\n",
       "      <td>147.797333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>abuts</td>\n",
       "      <td>462</td>\n",
       "      <td>87.080866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ablet</td>\n",
       "      <td>462</td>\n",
       "      <td>69.274591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>abies</td>\n",
       "      <td>469</td>\n",
       "      <td>45.670598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>abord</td>\n",
       "      <td>471</td>\n",
       "      <td>100.652328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>adder</td>\n",
       "      <td>474</td>\n",
       "      <td>119.786617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aarti</td>\n",
       "      <td>478</td>\n",
       "      <td>118.433935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>acing</td>\n",
       "      <td>478</td>\n",
       "      <td>131.195498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>abcee</td>\n",
       "      <td>481</td>\n",
       "      <td>137.204363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>absit</td>\n",
       "      <td>485</td>\n",
       "      <td>83.637835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>abyes</td>\n",
       "      <td>488</td>\n",
       "      <td>55.792014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>acais</td>\n",
       "      <td>493</td>\n",
       "      <td>130.602451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>acred</td>\n",
       "      <td>498</td>\n",
       "      <td>72.285230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>adown</td>\n",
       "      <td>501</td>\n",
       "      <td>126.510870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>adsum</td>\n",
       "      <td>502</td>\n",
       "      <td>108.025825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>acned</td>\n",
       "      <td>505</td>\n",
       "      <td>79.917515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>ackee</td>\n",
       "      <td>506</td>\n",
       "      <td>147.287542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>adeem</td>\n",
       "      <td>508</td>\n",
       "      <td>152.381437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>admen</td>\n",
       "      <td>509</td>\n",
       "      <td>87.133750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>abris</td>\n",
       "      <td>514</td>\n",
       "      <td>55.938560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ablow</td>\n",
       "      <td>518</td>\n",
       "      <td>134.667592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>abbed</td>\n",
       "      <td>523</td>\n",
       "      <td>156.094049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>adits</td>\n",
       "      <td>527</td>\n",
       "      <td>59.942954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ables</td>\n",
       "      <td>534</td>\n",
       "      <td>54.243447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  ncands_max  ncands_mean\n",
       "43  abune         344    66.645698\n",
       "30  aboil         348    63.762026\n",
       "54  aceta         359   114.152559\n",
       "75  actin         378    99.115171\n",
       "59  acids         383    64.878122\n",
       "84  addio         390    82.710762\n",
       "56  ached         390    91.699198\n",
       "88  adieu         394    48.431545\n",
       "70  acold         400    96.866019\n",
       "98  adred         409   110.192800\n",
       "16  abbes         423    89.563136\n",
       "49  acari         426   154.538545\n",
       "77  acyls         427    80.046099\n",
       "68  acnes         427    48.570459\n",
       "74  acted         428    70.731190\n",
       "40  absey         430    68.383287\n",
       "19  abear         431   138.623034\n",
       "85  addle         434   121.280296\n",
       "80  adbot         434   100.679078\n",
       "38  abrin         436    87.739053\n",
       "34  abore         437    63.105535\n",
       "0   aahed         448   146.204595\n",
       "37  abrim         454   101.562365\n",
       "96  adoze         454    78.151249\n",
       "60  acidy         455   116.479494\n",
       "65  acmes         455    57.047410\n",
       "20  abele         457   147.797333\n",
       "44  abuts         462    87.080866\n",
       "26  ablet         462    69.274591\n",
       "23  abies         469    45.670598\n",
       "33  abord         471   100.652328\n",
       "83  adder         474   119.786617\n",
       "3   aarti         478   118.433935\n",
       "61  acing         478   131.195498\n",
       "17  abcee         481   137.204363\n",
       "41  absit         485    83.637835\n",
       "46  abyes         488    55.792014\n",
       "48  acais         493   130.602451\n",
       "71  acred         498    72.285230\n",
       "95  adown         501   126.510870\n",
       "99  adsum         502   108.025825\n",
       "67  acned         505    79.917515\n",
       "63  ackee         506   147.287542\n",
       "86  adeem         508   152.381437\n",
       "92  admen         509    87.133750\n",
       "39  abris         514    55.938560\n",
       "27  ablow         518   134.667592\n",
       "15  abbed         523   156.094049\n",
       "90  adits         527    59.942954\n",
       "25  ables         534    54.243447"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(test2, columns=['word', 'ncands_max', 'ncands_mean'])\n",
    "df2.sort_values('ncands_max').head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a041cdbd-259e-43a3-bd65-de56d3dc81d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8a59ce2-182c-4531-ae0d-6471b0a537d6",
   "metadata": {},
   "source": [
    "## App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aca58ebd-b0c9-4aa3-afeb-c72121ec868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_app(pre_load=None):\n",
    "    candidate_mask = init_candidate_mask()\n",
    "    filter_mask = init_filter_mask()\n",
    "    step = 1\n",
    "    w = wordle.copy()\n",
    "    res = pd.DataFrame([{'a': 1}, {'a': 1}])\n",
    "    tested_words = []\n",
    "    all_chars = []\n",
    "    \n",
    "    while res.shape[0] > 1:\n",
    "        print(f'[ ---- STEP {step} ----]')\n",
    "        if not (step == 1 and pre_load):\n",
    "            guess = input('Input a guess:')\n",
    "        else:\n",
    "            guess = pre_load\n",
    "        if guess.lower() in ['quit', 'q']:\n",
    "            return w, candidate_mask, res\n",
    "        tested_words.append(guess)\n",
    "        all_chars = all_chars + list(set(guess))\n",
    "        all_chars = list(set(all_chars))\n",
    "        \n",
    "        fb = input('Input feedback:')\n",
    "        if fb.lower() in ['quit', 'q']:\n",
    "            return w, candidate_mask, res\n",
    "        \n",
    "        # Update masks and candidate set\n",
    "        candidate_mask = update_candidate_mask(guess, fb.upper(), wordle, candidate_mask)\n",
    "        filter_mask = update_filter_mask(guess, fb, filter_mask)\n",
    "        w = filter_wordset(guess, fb.upper(), w)\n",
    "        \n",
    "        # Candidates\n",
    "        print(f'Found {w.shape[0]} candidates. Running analysis...')\n",
    "        new_scores = []\n",
    "        new_ncands = []\n",
    "        \n",
    "        # eval_mat = solutions[candidate_mask].copy()\n",
    "        \n",
    "        for word in tqdm(w.word):\n",
    "            # Get G/Y/X scores\n",
    "            new_scores.append(get_final_scores(word, candidate_mask))\n",
    "            \n",
    "            # Get projected filter mask\n",
    "#             temp_cands = 0\n",
    "#             for solution in w.word:\n",
    "#                 proj_fb = get_feedback(word, solution)\n",
    "#                 proj_filter_mask = update_filter_mask(word, proj_fb, filter_mask)\n",
    "#                 temp_cands += get_n_cands(word, proj_filter_mask, candidate_mask)\n",
    "            \n",
    "#             new_ncands.append(np.mean(temp_cands / w.shape[0]))\n",
    "            \n",
    "        \n",
    "        print(f'Suggestions for step {step + 1} ({w.shape[0]}):')\n",
    "        res = pd.DataFrame({'word': w.word, 'score': new_scores}) \\\n",
    "            .merge(words_all[['word', 'word_freq']], on='word', how='left') \\\n",
    "            .fillna(0)\n",
    "        # display(res.sort_values(['ncands', 'score', 'word_freq'], ascending=[True, False, False]).head(10))\n",
    "        display(res.sort_values(['score', 'word_freq'], ascending=False).head(10))\n",
    "        \n",
    "        # Filters\n",
    "        # if w.shape[0] > 10:\n",
    "        #     print(f'\\nLarge number of candidates found ({w.shape[0]}). We recommend filtering the candidates more:')\n",
    "        #     display(\n",
    "        #         global_scores.loc[global_scores.word.apply(lambda x: len(set(all_chars).intersection(set(list(x)))) < 1)] \\\n",
    "        #             .head(5)\n",
    "        #     )\n",
    "        if w.shape[0] <= 10:\n",
    "            print(f'Small number of candidates remaining ({w.shape[0]}). We recommend choosing the most popular option:')\n",
    "            display(res.sort_values(['word_freq', 'score'], ascending=False).head(5))\n",
    "        \n",
    "        # Check for repeats\n",
    "        if w.shape[0] <= 8 and w.shape[0] >= 3:\n",
    "            w_copy = res.sort_values(['word_freq', 'score'], ascending=False).copy()\n",
    "            # Extract letters\n",
    "            for i in range(5):\n",
    "                w_copy[f'p{i}'] = w_copy.word.str[i]\n",
    "            \n",
    "            # Count the number of unique columns\n",
    "            unique_mask = w_copy.iloc[:, -5:].nunique() > 1\n",
    "            \n",
    "            # If only 1, then recommend another word\n",
    "            if unique_mask.sum() == 1:\n",
    "                wc = wordle.copy()\n",
    "                total_letters = w_copy.shape[0]\n",
    "                wc['scores'] = 0\n",
    "                wc['counts'] = 0\n",
    "                for i, letter in enumerate(np.squeeze(w_copy[unique_mask.index[unique_mask]].values)):\n",
    "                    wc['scores'] = wc['scores'] + (total_letters - i) * wc.word.str.contains(letter).astype(int)\n",
    "                    wc['counts'] = wc['counts'] + wc.word.str.contains(letter).astype(int)\n",
    "                    \n",
    "                print(f'\\nWords with only one letter differential detected. Consider filtering:')\n",
    "                display(wc.loc[wc.counts.le(total_letters // 2 * 3)].sort_values('scores', ascending=False).head(5))\n",
    "\n",
    "                \n",
    "        step += 1\n",
    "        print()\n",
    "        \n",
    "    return w, candidate_mask, res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff5561-841b-4e9e-a22c-8284d2c4163f",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fda297e-70bd-4c02-b498-e2eecea8b7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ---- STEP 1 ----]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input feedback: xxxyy\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 2315 but corresponding boolean dimension is 12972",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21160/3093016217.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcurr_wordset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_app\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'soare'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_21160/1554989532.py\u001b[0m in \u001b[0;36mrun_app\u001b[0;34m(pre_load)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Update masks and candidate set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mcandidate_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_candidate_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mfilter_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_filter_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_wordset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_21160/2057371689.py\u001b[0m in \u001b[0;36mupdate_candidate_mask\u001b[0;34m(input_word, feedback, wordset, mask)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mnewmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mwordset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mfeedback\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'X'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mnewmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwordset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnewmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 2315 but corresponding boolean dimension is 12972"
     ]
    }
   ],
   "source": [
    "curr_wordset, curr_mask, curr_res = run_app('soare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237e9ce4-9597-4b6a-9adc-fbb02de65d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
